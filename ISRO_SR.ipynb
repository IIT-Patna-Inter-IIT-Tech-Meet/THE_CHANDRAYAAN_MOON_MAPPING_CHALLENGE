{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["*README*\n","1. Run the Model cell. </br>\n","2. Run the Inference cell. </br>\n","3. After running the inference cell , follow the example cell. </br>\n","4. In the example cell, a 2d numpy array which is in low resolution is passed to the final_model. Before passing the array, make sure to apply min-max standardization (to convert the pixel values between 0 and 1) on the image as shown in example. </br>\n","5. The output is the 32x super resolved image which is in the form of a 2d numpy array. </br>\n","</br>\n","Note - The output image dimension will not be exactly 32 times. The input image is truncated so that both the dimensions are multiples of 80 inside the inference model.\n","</br>\n","Models state dictionaries for both stages of the model are provided in the same folder as the notebook.\n"],"metadata":{"id":"T1Y_S_-MKJ02"}},{"cell_type":"markdown","source":["**Required libraries**</br>\n","PyTorch 1.13.1 </br>\n","NumPy 1.21.6 </br>\n","torchmetrics </br>\n","Pillow </br>\n","OpenCV </br>"],"metadata":{"id":"r-YSmd8vJLqw"}},{"cell_type":"code","source":["!pip list "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xprbDMzqJzT2","outputId":"5f17b96a-c0a1-4b96-eee4-38b68a5e7b36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Package                       Version\n","----------------------------- ----------------------\n","absl-py                       1.4.0\n","aeppl                         0.0.33\n","aesara                        2.7.9\n","aiohttp                       3.8.3\n","aiosignal                     1.3.1\n","alabaster                     0.7.13\n","albumentations                1.2.1\n","altair                        4.2.2\n","appdirs                       1.4.4\n","arviz                         0.12.1\n","astor                         0.8.1\n","astropy                       4.3.1\n","astunparse                    1.6.3\n","async-timeout                 4.0.2\n","atari-py                      0.2.9\n","atomicwrites                  1.4.1\n","attrs                         22.2.0\n","audioread                     3.0.0\n","autograd                      1.5\n","Babel                         2.11.0\n","backcall                      0.2.0\n","beautifulsoup4                4.6.3\n","bleach                        6.0.0\n","blis                          0.7.9\n","bokeh                         2.3.3\n","branca                        0.6.0\n","bs4                           0.0.1\n","CacheControl                  0.12.11\n","cachetools                    5.3.0\n","catalogue                     2.0.8\n","certifi                       2022.12.7\n","cffi                          1.15.1\n","cftime                        1.6.2\n","chardet                       4.0.0\n","charset-normalizer            2.1.1\n","click                         7.1.2\n","clikit                        0.6.2\n","cloudpickle                   2.2.1\n","cmake                         3.22.6\n","cmdstanpy                     1.1.0\n","colorcet                      3.0.1\n","colorlover                    0.3.0\n","community                     1.0.0b1\n","confection                    0.0.4\n","cons                          0.4.5\n","contextlib2                   0.5.5\n","convertdate                   2.4.0\n","crashtest                     0.3.1\n","crcmod                        1.7\n","cufflinks                     0.17.3\n","cupy-cuda11x                  11.0.0\n","cvxopt                        1.3.0\n","cvxpy                         1.2.3\n","cycler                        0.11.0\n","cymem                         2.0.7\n","Cython                        0.29.33\n","daft                          0.0.4\n","dask                          2022.2.1\n","datascience                   0.17.5\n","db-dtypes                     1.0.5\n","dbus-python                   1.2.16\n","debugpy                       1.0.0\n","decorator                     4.4.2\n","defusedxml                    0.7.1\n","descartes                     1.1.0\n","dill                          0.3.6\n","distributed                   2022.2.1\n","dlib                          19.24.0\n","dm-tree                       0.1.8\n","dnspython                     2.3.0\n","docutils                      0.16\n","dopamine-rl                   1.0.5\n","earthengine-api               0.1.339\n","easydict                      1.10\n","ecos                          2.0.12\n","editdistance                  0.5.3\n","en-core-web-sm                3.4.1\n","entrypoints                   0.4\n","ephem                         4.1.4\n","et-xmlfile                    1.1.0\n","etils                         1.0.0\n","etuples                       0.3.8\n","fa2                           0.3.5\n","fastai                        2.7.10\n","fastcore                      1.5.28\n","fastdownload                  0.0.7\n","fastdtw                       0.3.4\n","fastjsonschema                2.16.2\n","fastprogress                  1.0.3\n","fastrlock                     0.8.1\n","feather-format                0.4.1\n","filelock                      3.9.0\n","firebase-admin                5.3.0\n","fix-yahoo-finance             0.0.22\n","Flask                         1.1.4\n","flatbuffers                   1.12\n","folium                        0.12.1.post1\n","frozenlist                    1.3.3\n","fsspec                        2023.1.0\n","future                        0.16.0\n","gast                          0.4.0\n","GDAL                          3.0.4\n","gdown                         4.4.0\n","gensim                        3.6.0\n","geographiclib                 1.52\n","geopy                         1.17.0\n","gin-config                    0.5.0\n","glob2                         0.7\n","google                        2.0.3\n","google-api-core               2.11.0\n","google-api-python-client      2.70.0\n","google-auth                   2.16.0\n","google-auth-httplib2          0.1.0\n","google-auth-oauthlib          0.4.6\n","google-cloud-bigquery         3.4.2\n","google-cloud-bigquery-storage 2.18.1\n","google-cloud-core             2.3.2\n","google-cloud-datastore        2.11.1\n","google-cloud-firestore        2.7.3\n","google-cloud-language         2.6.1\n","google-cloud-storage          2.7.0\n","google-cloud-translate        3.8.4\n","google-colab                  1.0.0\n","google-crc32c                 1.5.0\n","google-pasta                  0.2.0\n","google-resumable-media        2.4.1\n","googleapis-common-protos      1.58.0\n","googledrivedownloader         0.4\n","graphviz                      0.10.1\n","greenlet                      2.0.2\n","grpcio                        1.51.1\n","grpcio-status                 1.48.2\n","gspread                       3.4.2\n","gspread-dataframe             3.0.8\n","gym                           0.25.2\n","gym-notices                   0.0.8\n","h5py                          3.1.0\n","HeapDict                      1.0.1\n","hijri-converter               2.2.4\n","holidays                      0.19\n","holoviews                     1.14.9\n","html5lib                      1.0.1\n","httpimport                    0.5.18\n","httplib2                      0.17.4\n","httpstan                      4.6.1\n","humanize                      0.5.1\n","hyperopt                      0.1.2\n","idna                          2.10\n","imageio                       2.9.0\n","imagesize                     1.4.1\n","imbalanced-learn              0.8.1\n","imblearn                      0.0\n","imgaug                        0.4.0\n","importlib-metadata            6.0.0\n","importlib-resources           5.10.2\n","imutils                       0.5.4\n","inflect                       2.1.0\n","intel-openmp                  2023.0.0\n","intervaltree                  2.1.0\n","ipykernel                     5.3.4\n","ipython                       7.9.0\n","ipython-genutils              0.2.0\n","ipython-sql                   0.3.9\n","ipywidgets                    7.7.1\n","itsdangerous                  1.1.0\n","jax                           0.3.25\n","jaxlib                        0.3.25+cuda11.cudnn805\n","jieba                         0.42.1\n","Jinja2                        2.11.3\n","joblib                        1.2.0\n","jpeg4py                       0.1.4\n","jsonschema                    4.3.3\n","jupyter-client                6.1.12\n","jupyter-console               6.1.0\n","jupyter_core                  5.2.0\n","jupyterlab-widgets            3.0.5\n","kaggle                        1.5.12\n","kapre                         0.3.7\n","keras                         2.9.0\n","Keras-Preprocessing           1.1.2\n","keras-vis                     0.4.1\n","kiwisolver                    1.4.4\n","korean-lunar-calendar         0.3.1\n","langcodes                     3.3.0\n","libclang                      15.0.6.1\n","librosa                       0.8.1\n","lightgbm                      2.2.3\n","llvmlite                      0.39.1\n","lmdb                          0.99\n","locket                        1.0.0\n","logical-unification           0.4.5\n","LunarCalendar                 0.0.9\n","lxml                          4.9.2\n","Markdown                      3.4.1\n","MarkupSafe                    2.0.1\n","marshmallow                   3.19.0\n","matplotlib                    3.2.2\n","matplotlib-venn               0.11.7\n","miniKanren                    1.0.3\n","missingno                     0.5.1\n","mistune                       0.8.4\n","mizani                        0.7.3\n","mkl                           2019.0\n","mlxtend                       0.14.0\n","more-itertools                9.0.0\n","moviepy                       0.2.3.5\n","mpmath                        1.2.1\n","msgpack                       1.0.4\n","multidict                     6.0.4\n","multipledispatch              0.6.0\n","multitasking                  0.0.11\n","murmurhash                    1.0.9\n","music21                       5.5.0\n","natsort                       5.5.0\n","nbconvert                     5.6.1\n","nbformat                      5.7.3\n","netCDF4                       1.6.2\n","networkx                      3.0\n","nibabel                       3.0.2\n","nltk                          3.7\n","notebook                      5.7.16\n","numba                         0.56.4\n","numexpr                       2.8.4\n","numpy                         1.21.6\n","oauth2client                  4.1.3\n","oauthlib                      3.2.2\n","okgrade                       0.4.3\n","opencv-contrib-python         4.6.0.66\n","opencv-python                 4.6.0.66\n","opencv-python-headless        4.7.0.68\n","openpyxl                      3.0.10\n","opt-einsum                    3.3.0\n","osqp                          0.6.2.post0\n","packaging                     23.0\n","palettable                    3.3.0\n","pandas                        1.3.5\n","pandas-datareader             0.9.0\n","pandas-gbq                    0.17.9\n","pandas-profiling              1.4.1\n","pandocfilters                 1.5.0\n","panel                         0.12.1\n","param                         1.12.3\n","parso                         0.8.3\n","partd                         1.3.0\n","pastel                        0.2.1\n","pathlib                       1.0.1\n","pathy                         0.10.1\n","patsy                         0.5.3\n","pds4-tools                    1.3\n","pep517                        0.13.0\n","pexpect                       4.8.0\n","pickleshare                   0.7.5\n","Pillow                        7.1.2\n","pip                           22.0.4\n","pip-tools                     6.6.2\n","platformdirs                  2.6.2\n","plotly                        5.5.0\n","plotnine                      0.8.0\n","pluggy                        0.7.1\n","pooch                         1.6.0\n","portpicker                    1.3.9\n","prefetch-generator            1.0.3\n","preshed                       3.0.8\n","prettytable                   3.6.0\n","progressbar2                  3.38.0\n","prometheus-client             0.16.0\n","promise                       2.3\n","prompt-toolkit                2.0.10\n","prophet                       1.1.2\n","proto-plus                    1.22.2\n","protobuf                      3.19.6\n","psutil                        5.4.8\n","psycopg2                      2.9.5\n","ptyprocess                    0.7.0\n","py                            1.11.0\n","pyarrow                       9.0.0\n","pyasn1                        0.4.8\n","pyasn1-modules                0.2.8\n","pycocotools                   2.0.6\n","pycparser                     2.21\n","pyct                          0.5.0\n","pydantic                      1.10.4\n","pydata-google-auth            1.6.0\n","pydot                         1.3.0\n","pydot-ng                      2.0.0\n","pydotplus                     2.0.2\n","PyDrive                       1.3.1\n","pyemd                         0.5.1\n","pyerfa                        2.0.0.1\n","Pygments                      2.6.1\n","PyGObject                     3.36.0\n","pylev                         1.4.0\n","pymc                          4.1.4\n","PyMeeus                       0.5.12\n","pymongo                       4.3.3\n","pymystem3                     0.2.0\n","PyOpenGL                      3.1.6\n","pyparsing                     3.0.9\n","pyrsistent                    0.19.3\n","pysimdjson                    3.2.0\n","PySocks                       1.7.1\n","pystan                        3.3.0\n","pytest                        3.6.4\n","python-apt                    2.0.1\n","python-dateutil               2.8.2\n","python-louvain                0.16\n","python-slugify                8.0.0\n","python-utils                  3.4.5\n","pytz                          2022.7.1\n","pyviz-comms                   2.2.1\n","PyWavelets                    1.4.1\n","PyYAML                        6.0\n","pyzmq                         23.2.1\n","qdldl                         0.1.5.post3\n","qudida                        0.0.4\n","regex                         2022.6.2\n","requests                      2.25.1\n","requests-oauthlib             1.3.1\n","requests-unixsocket           0.2.0\n","resampy                       0.4.2\n","rpy2                          3.5.5\n","rsa                           4.9\n","scikit-image                  0.18.3\n","scikit-learn                  1.0.2\n","scipy                         1.7.3\n","screen-resolution-extra       0.0.0\n","scs                           3.2.2\n","seaborn                       0.11.2\n","Send2Trash                    1.8.0\n","setuptools                    57.4.0\n","shapely                       2.0.1\n","six                           1.15.0\n","sklearn-pandas                1.8.0\n","smart-open                    6.3.0\n","snowballstemmer               2.2.0\n","sortedcontainers              2.4.0\n","soundfile                     0.11.0\n","spacy                         3.4.4\n","spacy-legacy                  3.0.12\n","spacy-loggers                 1.0.4\n","Sphinx                        3.5.4\n","sphinxcontrib-applehelp       1.0.4\n","sphinxcontrib-devhelp         1.0.2\n","sphinxcontrib-htmlhelp        2.0.1\n","sphinxcontrib-jsmath          1.0.1\n","sphinxcontrib-qthelp          1.0.3\n","sphinxcontrib-serializinghtml 1.1.5\n","SQLAlchemy                    1.4.46\n","sqlparse                      0.4.3\n","srsly                         2.4.5\n","statsmodels                   0.12.2\n","sympy                         1.7.1\n","tables                        3.7.0\n","tabulate                      0.8.10\n","tblib                         1.7.0\n","tenacity                      8.1.0\n","tensorboard                   2.9.1\n","tensorboard-data-server       0.6.1\n","tensorboard-plugin-wit        1.8.1\n","tensorflow                    2.9.2\n","tensorflow-datasets           4.8.2\n","tensorflow-estimator          2.9.0\n","tensorflow-gcs-config         2.9.1\n","tensorflow-hub                0.12.0\n","tensorflow-io-gcs-filesystem  0.30.0\n","tensorflow-metadata           1.12.0\n","tensorflow-probability        0.17.0\n","termcolor                     2.2.0\n","terminado                     0.13.3\n","testpath                      0.6.0\n","text-unidecode                1.3\n","textblob                      0.15.3\n","thinc                         8.1.7\n","threadpoolctl                 3.1.0\n","tifffile                      2023.1.23.1\n","toml                          0.10.2\n","tomli                         2.0.1\n","toolz                         0.12.0\n","torch                         1.13.1+cu116\n","torchaudio                    0.13.1+cu116\n","torchsummary                  1.5.1\n","torchtext                     0.14.1\n","torchvision                   0.14.1+cu116\n","tornado                       6.0.4\n","tqdm                          4.64.1\n","traitlets                     5.7.1\n","tweepy                        3.10.0\n","typeguard                     2.7.1\n","typer                         0.7.0\n","typing_extensions             4.4.0\n","tzlocal                       1.5.1\n","uritemplate                   4.1.1\n","urllib3                       1.24.3\n","vega-datasets                 0.9.0\n","wasabi                        0.10.1\n","wcwidth                       0.2.6\n","webargs                       8.2.0\n","webencodings                  0.5.1\n","Werkzeug                      1.0.1\n","wheel                         0.38.4\n","widgetsnbextension            3.6.1\n","wordcloud                     1.8.2.2\n","wrapt                         1.14.1\n","xarray                        2022.12.0\n","xarray-einstats               0.5.1\n","xgboost                       0.90\n","xkit                          0.0.0\n","xlrd                          1.2.0\n","xlwt                          1.3.0\n","yarl                          1.8.2\n","yellowbrick                   1.5\n","zict                          2.2.0\n","zipp                          3.12.0\n"]}]},{"cell_type":"markdown","source":["**Model**</br>\n","Definiton of the CSASR model which we have used for super resolution"],"metadata":{"id":"-CypRGrgDojs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrU-bjymDEh6"},"outputs":[],"source":["import math\n","from argparse import Namespace\n","import torch\n","import torch.nn as nn\n","from torch.nn.parameter import Parameter\n","import torch.nn.functional as F\n","from collections import OrderedDict\n","import numpy as np\n","import random\n","# from models import MPNCOV\n","\n","\n","def conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True,\n","               pad_type='zero', norm_type=None, act_type='relu'):\n","    padding = get_valid_padding(kernel_size, dilation)\n","    # p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n","    padding = padding if pad_type == 'zero' else 0\n","\n","    c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding,\n","                  dilation=dilation, bias=bias, groups=groups)\n","    a = activation(act_type) if act_type else None\n","    n = norm(norm_type, out_nc) if norm_type else None\n","    # return sequential(p, c, n, a)\n","    return sequential(c, n, a)\n","\n","\n","def norm(norm_type, out_nc):\n","    norm_type = norm_type.lower()\n","    if norm_type == 'bn':\n","        layer = nn.BatchNorm2d(out_nc)\n","    elif norm_type == 'in':\n","        layer = nn.InstanceNorm2d(out_nc)\n","    elif norm_type == 'gn':\n","        layer = nn.GroupNorm(4, out_nc)\n","    else:\n","        raise NotImplementedError('activation layer [{:s}] is not found'.format(norm_type))\n","    return layer\n","\n","\n","def activation(act_type, inplace=True, neg_slope=0.05, n_prelu=1):\n","    act_type = act_type.lower()\n","    if act_type == 'relu':\n","        layer = nn.ReLU(inplace)\n","    elif act_type == 'lrelu':\n","        layer = nn.LeakyReLU(neg_slope, inplace)\n","    elif act_type == 'prelu':\n","        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n","    else:\n","        raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n","    return layer\n","\n","\n","def sequential(*args):\n","    if len(args) == 1:\n","        if isinstance(args[0], OrderedDict):\n","            raise NotImplementedError('sequential does not support OrderedDict input.')\n","        return args[0]\n","    modules = []\n","    for module in args:\n","        if isinstance(module, nn.Sequential):\n","            for submodule in module.children():\n","                modules.append(submodule)\n","        elif isinstance(module, nn.Module):\n","            modules.append(module)\n","    return nn.Sequential(*modules)\n","\n","\n","def get_valid_padding(kernel_size, dilation):\n","    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n","    padding = (kernel_size - 1) // 2\n","    return padding\n","\n","\n","class Scale(nn.Module):\n","\n","    def __init__(self, init_value=1e-3):\n","        super().__init__()\n","        self.scale = nn.Parameter(torch.FloatTensor([init_value]))\n","\n","    def forward(self, input):\n","        return input * self.scale\n","\n","\n","class Tail(nn.Module):\n","    def __init__(self, args, scale, n_feats, kernel_size, wn):\n","        super(Tail, self).__init__()\n","        out_feats = scale * scale * args.n_colors\n","        # self.tail_k3 = wn(nn.Conv2d(n_feats, out_feats, 3, padding=3//2, dilation=1))\n","        # self.tail_k5 = wn(nn.Conv2d(n_feats, out_feats, 5, padding=5//2, dilation=1))\n","        self.tail_k3 = nn.Conv2d(n_feats, out_feats, 3, padding=3 // 2, dilation=1)\n","        self.tail_k5 = nn.Conv2d(n_feats, out_feats, 5, padding=5 // 2, dilation=1)\n","        self.pixelshuffle = nn.PixelShuffle(scale)\n","        self.scale_k3 = Scale(0.5)\n","        self.scale_k5 = Scale(0.5)\n","        self.conv = nn.Conv2d(n_feats, args.n_colors, kernel_size, padding=1)\n","\n","    def forward(self, x):\n","        x0 = self.pixelshuffle(self.scale_k3(self.tail_k3(x)))\n","        x1 = self.pixelshuffle(self.scale_k5(self.tail_k5(x)))\n","\n","        return x0 + x1\n","\n","\n","def pixel_unshuffle(input, downscale_factor):\n","    '''\n","    input: batchSize * c * k*w * k*h\n","    kdownscale_factor: k\n","    batchSize * c * k*w * k*h -> batchSize * k*k*c * w * h\n","    '''\n","    c = input.shape[1]\n","\n","    kernel = torch.zeros(size=[downscale_factor * downscale_factor * c,\n","                               1, downscale_factor, downscale_factor],\n","                         device=input.device)\n","    for y in range(downscale_factor):\n","        for x in range(downscale_factor):\n","            kernel[x + y * downscale_factor::downscale_factor * downscale_factor, 0, y, x] = 1\n","    return F.conv2d(input, kernel, stride=downscale_factor, groups=c)\n","\n","\n","def channel_shuffle(x, groups):\n","    batchsize, num_channels, height, width = x.data.size()\n","    channels_per_group = num_channels // groups\n","    # reshape\n","    x = x.view(batchsize, groups,\n","               channels_per_group, height, width)\n","    # transpose\n","    # - contiguous() required if transpose() is used before view().\n","    #   See https://github.com/pytorch/pytorch/issues/764\n","    x = torch.transpose(x, 1, 2).contiguous()\n","    # flatten\n","    x = x.view(batchsize, -1, height, width)\n","    return x\n","\n","\n","class FFB(nn.Module):\n","    def __init__(self, n_feats, wn, act=nn.ReLU(True)):\n","        super(FFB, self).__init__()\n","\n","        self.b0 = CSA(n_feats=n_feats, reduction_factor=4)\n","        self.b1 = CSA(n_feats=n_feats, reduction_factor=4)\n","        self.b2 = CSA(n_feats=n_feats, reduction_factor=4)\n","        self.b3 = CSA(n_feats=n_feats, reduction_factor=4)\n","\n","        # self.reduction1 = wn(nn.Conv2d(n_feats*2, n_feats, 1))\n","        # self.reduction2 = wn(nn.Conv2d(n_feats*2, n_feats, 1))\n","        # self.reduction3 = wn(nn.Conv2d(n_feats*2, n_feats, 1))\n","        self.reduction1 = nn.Conv2d(n_feats * 2, n_feats, 1)\n","        self.reduction2 = nn.Conv2d(n_feats * 2, n_feats, 1)\n","        self.reduction3 = nn.Conv2d(n_feats * 2, n_feats, 1)\n","        self.res_scale = Scale(1)\n","        self.x_scale = Scale(1)\n","\n","    def forward(self, x):\n","        x0 = self.b0(x)\n","        x1 = self.b1(x0) + x0\n","        x2 = self.b2(x1) + x1\n","        x3 = self.b3(x2)\n","\n","        res1 = self.reduction1(channel_shuffle(torch.cat([x0, x3], dim=1), 2))\n","        res2 = self.reduction2(channel_shuffle(torch.cat([x1, x2], dim=1), 2))\n","        res = self.reduction3(channel_shuffle(torch.cat([res1, res2], dim=1), 2))\n","\n","        return self.res_scale(res) + self.x_scale(x)\n","\n","class ECA(nn.Module):\n","    def __init__(self, channel, k_size=3):\n","        super(ECA, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # feature descriptor on the global spatial information\n","        y = self.avg_pool(x)\n","\n","        # Two different branches of ECA module\n","        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n","\n","        # Multi-scale information fusion\n","        y = self.sigmoid(y)\n","\n","        return x * y.expand_as(x)\n","\n","class ESA(nn.Module):\n","    def __init__(self, n_feats, reduction_factor=4, distillation_rate=0.25):\n","        super(ESA, self).__init__()\n","        self.reduce_channels = nn.Conv2d(n_feats, n_feats // reduction_factor, 1)\n","        self.reduce_spatial_size = nn.Conv2d(n_feats // reduction_factor, n_feats // reduction_factor, 3, stride=2,\n","                                             padding=1)\n","        self.pool = nn.MaxPool2d(7, stride=3)\n","        self.increase_channels = conv_block(n_feats // reduction_factor, n_feats, 1)\n","\n","        self.conv1 = conv_block(n_feats // reduction_factor, n_feats // reduction_factor, 3, dilation=1,\n","                                act_type='lrelu')\n","        self.conv2 = conv_block(n_feats // reduction_factor, n_feats // reduction_factor, 3, dilation=2,\n","                                act_type='lrelu')\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.bottom11 = conv_block(n_feats, n_feats, 1, act_type=None)\n","        self.bottom11_dw = conv_block(n_feats, n_feats, 5, groups=n_feats, act_type=None)\n","\n","    def forward(self, x):\n","        rc = self.reduce_channels(x)\n","        rs = self.reduce_spatial_size(rc)\n","        pool = self.pool(rs)\n","        # conv = self.conv2(pool)\n","        # conv = conv + self.conv1(pool)\n","        conv = self.conv2(pool)\n","        conv = self.conv1(conv)\n","        up = torch.nn.functional.upsample(conv, size=(rc.shape[2], rc.shape[3]), mode='nearest')\n","        up = up + rc\n","        # out = (self.sigmoid(self.increase_channels(up)) * x) * self.sigmoid(self.bottom11_dw(self.bottom11(x)))\n","        out = (self.sigmoid(self.increase_channels(up)) * x)\n","        return out\n","\n","\n","class CSA(nn.Module):\n","    def __init__(self, n_feats, reduction_factor=4, distillation_rate=0.25):\n","        super(CSA, self).__init__()\n","\n","        self.conv00 = conv_block(n_feats, n_feats, 3, act_type=None)\n","        self.conv01 = conv_block(n_feats, n_feats, 3, act_type='lrelu')\n","\n","        self.eca = ECA(n_feats)\n","        self.esa = ESA(n_feats, reduction_factor)\n","\n","    def forward(self, x):\n","        x = self.conv00(self.conv01(x))\n","        eca_ = self.eca(x)\n","        esa_ = self.esa(eca_)\n","        return esa_\n","\n","\n","class CSASR(nn.Module):\n","    def __init__(self, args):\n","        super(CSASR, self).__init__()\n","        self.args = args\n","\n","        # hyper-params\n","        self.scale = args.scale\n","        n_FFBs = args.n_FFBs\n","        n_feats = args.n_feats\n","        kernel_size = 3\n","        act = nn.LeakyReLU(True)\n","\n","        wn = lambda x: torch.nn.utils.weight_norm(x)\n","\n","        # self.rgb_mean = torch.autograd.Variable(torch.FloatTensor(\n","        #     [0.4488, 0.4371, 0.4040])).view([1, 3, 1, 1])\n","\n","        # define head module\n","        head = []\n","        # head.append(wn(nn.Conv2d(3, n_feats, 3, padding=3//2)))\n","        head.append(nn.Conv2d(3, n_feats, 3, padding=3 // 2))\n","\n","        # define body module\n","        body = []\n","        for i in range(n_FFBs):\n","            body.append(\n","                FFB(n_feats, wn=wn, act=act))\n","\n","        # make object members\n","        self.head = nn.Sequential(*head)\n","        self.body = nn.Sequential(*body)\n","\n","        if args.no_upsampling:\n","            self.out_dim = n_feats\n","        else:\n","            self.out_dim = args.n_colors\n","            # define tail module\n","            self.tail = Tail(args, self.scale, n_feats, kernel_size, wn)\n","\n","    def forward(self, x):\n","        input = x\n","        x = self.head(x)\n","        x = self.body(x)\n","        if self.args.no_upsampling:\n","            return x\n","        else:\n","            x = self.tail(x)\n","            return x + torch.nn.functional.upsample(input, scale_factor=self.scale, mode='bicubic')\n","\n","    def load_state_dict(self, state_dict, strict=True):\n","        own_state = self.state_dict()\n","        for name, param in state_dict.items():\n","            if name in own_state:\n","                if isinstance(param, nn.Parameter):\n","                    param = param.data\n","                try:\n","                    own_state[name].copy_(param)\n","                except Exception:\n","                    if name.find('tail') >= 0 or name.find('skip') >= 0:\n","                        print('Replace pre-trained upsampler to new one...')\n","                    else:\n","                        raise RuntimeError('While copying the parameter named {}, '\n","                                           'whose dimensions in the model are {} and '\n","                                           'whose dimensions in the checkpoint are {}.'\n","                                           .format(name, own_state[name].size(), param.size()))\n","            elif strict:\n","                if name.find('tail') == -1:\n","                    raise KeyError('unexpected key \"{}\" in state_dict'\n","                                   .format(name))\n","\n","        if strict:\n","            missing = set(own_state.keys()) - set(state_dict.keys())\n","            if len(missing) > 0:\n","                raise KeyError('missing keys in state_dict: \"{}\"'.format(missing))\n","\n","\n","def make_csasr(scale=4, n_FFBs=4, n_feats=32, no_upsampling=False):\n","    args = Namespace()\n","    args.scale = scale\n","    args.n_FFBs = n_FFBs\n","    args.n_feats = n_feats\n","    args.no_upsampling = no_upsampling  # True\n","    args.n_colors = 3\n","    return CSASR(args)"]},{"cell_type":"markdown","source":["**Dataset**</br>\n","Definition of the dataset class used for loading the images used for training the super resolution model.\n"],"metadata":{"id":"n22crIXzKxZ9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5p8D9LRpJH3X"},"outputs":[],"source":["from torch.utils.data import Dataset\n","import cv2\n","import torch\n","from torchvision import transforms\n","from PIL import Image\n","import PIL\n","import numpy as np\n","\n","# Dataset class for OHRC images\n","class OHRC_stage1(Dataset):\n","    def __init__(self, id,img_path):\n","        self.id = id\n","        self.img_path = img_path\n","        \n","    def __len__(self):\n","        return len(self.id)\n","    \n","    def __getitem__(self, index):\n","        id = self.id[index]\n","        img_lr=np.load(self.img_path+'LR/'+str(id)+'.npy')\n","        img_lr = np.repeat(img_lr[:, :, np.newaxis], 3, axis=2)\n","        img_lr=np.transpose(img_lr,[2,0,1])\n","        img_hr=np.load(self.img_path+'HR/'+str(id)+'.npy')\n","        img_hr = np.repeat(img_hr[:, :, np.newaxis], 3, axis=2)\n","        img_hr=np.transpose(img_hr,[2,0,1])\n","        img_hr=torch.Tensor(img_hr)\n","        img_lr=torch.Tensor(img_lr)\n","        img_hr=img_hr/255.0\n","        img_lr=img_lr/255.0\n","       \n","        return img_lr,img_hr"]},{"cell_type":"code","source":["# Load the training and testing dataset\n","import random\n","from sklearn.model_selection import train_test_split\n","n=6000  # length of dataset\n","data_ids=[i for i in range(n)]\n","data_train_ids,data_test_ids=train_test_split(data_ids,test_size=0.20)\n","data_training=OHRC_stage1(data_train_ids,'/content/drive/MyDrive/Dataset/Dataset1_deci/')\n","data_testing=OHRC_stage1(data_test_ids,'/content/drive/MyDrive/Dataset/Dataset1_deci/')"],"metadata":{"id":"YOQombJYMdwy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Trainer**\n"],"metadata":{"id":"1WhL4LQhDrgN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wF-jRmH7fUm2"},"outputs":[],"source":["from torch.utils.data import DataLoader, Dataset\n","from torch import nn\n","from tqdm import tqdm\n","import torch\n","import torch.optim as optim\n","from torchmetrics import StructuralSimilarityIndexMeasure\n","\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","\n","#custom training\n","#net->model(network)\n","class data_train:\n","\n","    def __init__(self, training_dataset, net, args):\n","\n","        self.training_dataset = training_dataset\n","        self.net = net\n","        self.args = args\n","        \n","        self.n_pool = len(training_dataset)\n","        \n","        if 'islogs' not in args:\n","            self.args['islogs'] = False\n","\n","        if 'optimizer' not in args:\n","            self.args['optimizer'] = 'sgd'\n","        \n","        if 'isreset' not in args:\n","            self.args['isreset'] = True\n","            \n","        if 'device' not in args:\n","            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        else:\n","            self.device = args['device']\n","\n","    # Evaluation metric for the model\n","    def evaluation_metric(self,predicted_image,actual_image, redn):\n","           if redn == None:\n","                 redn = \"elementwise_mean\"\n","           ssim = StructuralSimilarityIndexMeasure(reduction=redn)\n","           ssim=ssim.to(device=self.device)\n","           return ssim(predicted_image, actual_image)\n","    \n","    # Function to evaluate the model on test data\n","    def evaluate_on_test(self, loader_test):\n","        self.clf.eval()\n","        ssim = 0\n","        count = 0\n","        with torch.no_grad():  \n","          for batch_id, (x, y) in enumerate(loader_test):\n","            x, y = x.to(device=self.device), y.to(device=self.device)\n","            out = self.clf(x)\n","            count = count + x.size(dim=0)\n","            ssim = ssim + self.evaluation_metric(out,y, \"sum\")\n","        return  ssim/(count+1e-7)\n","\n","    # Squared loss error function\n","    def squared_loss(self,predicted_image,actual_image):\n","      loss=nn.MSELoss(reduction='mean')\n","      return loss(predicted_image,actual_image)\n","\n","    # Function to train the model on one loader\n","    def _train(self, epoch, loader_tr, optimizer):\n","        self.clf.train()\n","        \n","        for batch_id, (x, y) in enumerate(loader_tr):\n","            x, y = x.to(device=self.device), y.to(device=self.device)\n","            \n","            optimizer.zero_grad()\n","            out = self.clf(x)\n","\n","            loss=self.squared_loss(out,y)\n","            loss.backward() \n","            optimizer.step()\n","        return  loss\n","\n","    # Function to train the model\n","    def train(self):\n","\n","        print('Training..')\n","        def weight_reset(m):\n","            if hasattr(m, 'reset_parameters'):\n","                m.reset_parameters()\n","\n","        train_logs = []\n","        n_epoch = self.args['n_epoch']\n","        \n","        if self.args['isreset']:\n","            self.clf = self.net.apply(weight_reset).to(device=self.device)\n","        else:\n","            try:\n","                self.clf\n","            except:\n","                self.clf = self.net.apply(weight_reset).to(device=self.device)\n","\n","        if self.args['optimizer'] == 'sgd':\n","            optimizer = optim.SGD(self.clf.parameters(), lr = self.args['lr'], momentum=0.9, weight_decay=5e-4)\n","            lr_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)\n","        \n","        elif self.args['optimizer'] == 'adam':\n","            optimizer = optim.Adam(self.clf.parameters(), lr = self.args['lr'], weight_decay=0)\n","\n","        \n","        if 'batch_size' in self.args:\n","            batch_size = self.args['batch_size']\n","        else:\n","            batch_size = 1\n","\n","        # Set shuffle to true to encourage stochastic behavior for SGD\n","        loader_tr = DataLoader(self.training_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","        loader_test=DataLoader(data_testing, batch_size=batch_size, shuffle=False, pin_memory=True)\n","        \n","        epoch = 1\n","        \n","        with tqdm(range(n_epoch), position=0, leave=True) as pbar:\n","            while  (epoch < n_epoch) : \n","                \n","                lossCurrent = self._train(epoch, loader_tr, optimizer)\n","\n","                if self.args['optimizer'] == 'sgd':\n","                    lr_sched.step()\n","            \n","                epoch += 1\n","                pbar.set_postfix_str(F\" Loss: {lossCurrent}\")\n","                pbar.update()\n","                test_metric=self.evaluate_on_test(loader_test)\n","\n","                log_string = 'Epoch:' + str(epoch) +'- training loss: '+str(lossCurrent.item())+'testing_metric '+str(test_metric)\n","                print(log_string)\n","                train_logs.append(log_string)\n","\n","        if self.args['islogs']:\n","            return self.clf, train_logs\n","        else:\n","            return self.clf"]},{"cell_type":"code","source":["model=make_csasr()  # Define the model\n","\n","# Define the optimizer for training\n","train_args={'optimizer':'adam','device':'cuda','batch_size':128,'lr':0.0001,'n_epoch':400}\n","trainer=data_train(data_training,model,train_args) \n","\n","# Path for saving the trained model\n","model_path='/content/drive/MyDrive/Model/CSASR_model_stage1_state_dict'\n","\n","trained_model=trainer.train()                   # Train the model\n","trained_state_dict = trained_model.state_dict() # Extract the state dictionary\n","torch.save(trained_state_dict,model_path)       # Save the state dictionary for the model"],"metadata":{"id":"vNsPRTjCF9Y3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Inference**</br>\n","Code used for obtaining high resolution images from input low resolution images based on trained super resolution models. It returns a 32x super resolved image after passing through two consecutive super resolution models and a bicubic interpolation layer."],"metadata":{"id":"Jap4Dz0gFViz"}},{"cell_type":"code","source":["from torch.nn import Module\n","import cv2\n","\n","# Model to get 32x superresolved images. Forward propagate by passing the Low Resolution image as a 2d numpy array.\n","class Combined(Module):\n","    def __init__(self, model1_path, model2_path, args):\n","        super(Combined, self).__init__()\n","        \"\"\"\n","        Args:\n","          model1_path: Path to the first model\n","          model2_path: Path to the second model\n","          args: Arguments for the model\n","        \"\"\"\n","\n","        # Define the models\n","        self.model1 = make_csasr()\n","        self.model2 = make_csasr()\n","\n","        if 'device' not in args:\n","            args['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","        # Load state dict of trained models\n","        model1_state_dict = torch.load(model1_path, map_location=torch.device(args['device']))\n","        model2_state_dict = torch.load(model2_path, map_location=torch.device(args['device']))\n","        self.model1.load_state_dict(model1_state_dict)\n","        self.model2.load_state_dict(model2_state_dict)\n","\n","        # Move model to specified device\n","        self.model1=self.model1.to(device=args['device'])\n","        self.model2=self.model2.to(device=args['device'])\n","\n","        self.args = args\n","\n","        # Freeze the layers of the models\n","        for param in self.model1.parameters():\n","            param.requires_grad = False\n","        \n","        for param in self.model2.parameters():\n","            param.requires_grad = False\n","\n","    # Function to reshape the image into tiles\n","    def reshape_split(self, image: torch.Tensor, kernel_size: tuple):\n","        \"\"\"\n","        Args:\n","            image: torch.Tensor, shape = (img_height, img_width)\n","            kernel_size: tuple, (tile_height, tile_width)\n","        \"\"\"\n","\n","        img_height, img_width = image.shape\n","        channels = 1\n","        tile_height, tile_width = kernel_size\n","\n","        tiled_array = image.reshape(img_height // tile_height,\n","                                    tile_height,\n","                                    img_width // tile_width,\n","                                    tile_width,\n","                                    channels)   \n","        tiled_array = tiled_array.swapaxes(1, 2)    # (img_height//tile_height, img_width//tile_width, tile_height, tile_width, channels)\n","        return tiled_array\n","    \n","    # Function to tile the image\n","    def tile(self, image, tile_height, tile_width):\n","        \"\"\"\n","        Args:\n","            image: torch.Tensor, shape = (img_height, img_width)\n","            tile_height: int\n","            tile_width: int\n","        \"\"\"\n","        X = image.shape[-2] - (image.shape[0] % tile_height)\n","        Y = image.shape[-1] - (image.shape[1] % tile_width)\n","        image = image[:X, :Y]\n","        tiled_images = self.reshape_split(image, (tile_height, tile_width))\n","        tiled_images = torch.squeeze(tiled_images, dim = 4)\n","        tiled_images = torch.reshape(tiled_images, (tiled_images.shape[0] * tiled_images.shape[1], tiled_images.shape[2], tiled_images.shape[3]))\n","        return tiled_images\n","\n","\n","    # Function to stitch the tiles\n","    def stitch(self, tiles, tile_height, tile_width):\n","        \"\"\"\n","        Args:\n","            tiles: torch.Tensor (n_tiles, tile_height, tile_width)\n","            tile_height: int\n","            tile_width: int\n","        \"\"\"\n","        n_tiles = tiles.shape[0]\n","        img_height = tile_height * int(np.sqrt(n_tiles))\n","        img_width = tile_width * int(np.sqrt(n_tiles))\n","\n","        tiles = torch.reshape(tiles, (int(np.sqrt(n_tiles)), int(np.sqrt(n_tiles)), tile_height, tile_width))\n","        tiles = torch.swapaxes(tiles, 1, 2)\n","        image = torch.reshape(tiles, (img_height, img_width))\n","        return image\n","    \n","    # Pass a single tiled images through the model. Returns 16x high resolution image.\n","    def forward_prop(self, x):\n","        \"\"\"\n","        Args:\n","            x: torch.Tensor, shape = (img_height, img_width)\n","        \"\"\"\n","        img_height = x.shape[0]\n","        img_width = x.shape[1]\n","        x=torch.unsqueeze(x,dim=0)\n","        x=x.repeat(3,1,1)\n","\n","        x=x.unsqueeze(dim=0)\n","        out1 = self.model1(x)     # (1, 3, img_height*4, img_width*4)\n","        out1 = out1.mean(dim=1)  # (1, img_height*4, img_width*4)\n","        out1 = out1.squeeze(dim=0)  # (img_height*4, img_width*4)\n","        out1 = self.tile(out1, img_height, img_width)           # (16, img_height, img_width)\n","        out1 = out1.unsqueeze(dim=1)                            # (16, 1, img_height, img_width)\n","        out1 = out1.repeat(1, 3, 1, 1)                          # (16, 3, img_height, img_width)\n","        out2 = self.model2(out1)                                # (16, 3, img_height*4, img_width*4)\n","        out2 = out2.mean(dim=1)                                 # (16, img_height*4, img_width*4)\n","        out2 = self.stitch(out2, img_height*4, img_width*4)     # (img_height*16, img_width*16)\n","\n","        out2 = out2.unsqueeze(dim=0)\n","\n","        return out2\n","\n","    # Function for forward-propagation on the low resolution image to obtain 32x high resolution image\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: np.ndarray, shape = (img_height, img_width)\n","        \"\"\"\n","\n","        # Convert to torch.Tensor\n","        x = torch.from_numpy(x).float()\n","\n","        # Tile the image\n","        x = self.tile(x, 80, 80)    # (n_tiles, 80, 80)\n","\n","        out = []\n","        for img in x:\n","          out2 = self.forward_prop(img.to(self.args['device']))\n","          out.append(out2.cpu().detach())\n","\n","        out = torch.cat(out, dim=0)     # (n_tiles, 16*80, 16*80)\n","\n","        # Stitch the tiles\n","        out = self.stitch(out, 80*16, 80*16)    # (img_height*16, img_width*16)\n","\n","        # Convert to numpy\n","        out = out.numpy()\n","\n","        # Apply bicubic interpolation\n","        out = cv2.resize(out, (out.shape[0]*2, out.shape[1]*2), interpolation=cv2.INTER_CUBIC)  # (img_height*32, img_width*32)\n","\n","        return out\n","    "],"metadata":{"id":"GSS9RRZLjrkA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"iju5yNO0Ojte"}},{"cell_type":"code","source":["# Loading TMC images\n","!pip install pds4_tools\n","import pds4_tools\n","\n","path=\"/content/drive/MyDrive/unzipped_tmc/ch2_tmc_ndn_20220708T1314261067_d_dtm_d32.zip/data/derived/20220708/ch2_tmc_ndn_20220708T1314261067_d_dtm_d32.xml\"\n","#path_of_tmc_xml\n","def image_to_arr(path):\n","    structure=pds4_tools.read(path)\n","    \n","    return structure[0].data\n","\n","imgarr=(image_to_arr(path))#array of the tmc image"],"metadata":{"id":"BrR2cMyl41Gb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"512d86c8-dcb5-4ed2-ea02-872d2801b100"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pds4_tools in /usr/local/lib/python3.8/dist-packages (1.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pds4_tools) (1.21.6)\n","Processing label: /content/drive/MyDrive/unzipped_tmc/ch2_tmc_ndn_20220708T1314261067_d_dtm_d32.zip/data/derived/20220708/ch2_tmc_ndn_20220708T1314261067_d_dtm_d32.xml\n","Now processing a Array_2D_Image structure: ARRAY_0\n"]}]},{"cell_type":"markdown","source":["variable 'imgarr' will contain the 2d numpy array of image"],"metadata":{"id":"9r_Pi6UjOvzJ"}},{"cell_type":"code","source":["#imgarr is the 2d numpy array of low resolution image\n","imgarr=imgarr.astype('float32')\n","\n","# Function for min-max standardization\n","def min_max_standardization(imgarr):\n","  min = np.min(imgarr)\n","  max = np.max(imgarr)\n","  imgarr = (imgarr - min)/(max - min)\n","  return imgarr\n","\n","imgarr_new = min_max_standardization(imgarr)\n","\n","#Combined(model_stage1_path,model_stage2_path)\n","final_model=Combined('/content/drive/MyDrive/Model/CSASR_model_stage1_state_dict','/content/drive/MyDrive/Model/CSASR_model_stage1_state_dict',{'device':'cuda'})\n","output=final_model(imgarr_new)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"AfRhyNw-5XxH","outputId":"2b168222-389d-4f31-ca4e-eab0872662ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f81eb0f1760>"]},"metadata":{},"execution_count":4},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAEkAAAD8CAYAAAA7dIkaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXjc1XnvP2f2RcuM9tXarcU2tuUFYxPLtgx2TAIEHAJNIW3yhDZNmkByU3rvhVLo8nCb3lxunoTw0DQtYSmXgkOxARtwDDZeMJaQZWsdy5KskUbSyNJoRpp95tw/ZjSVkACNGiPPI77Po0czZ86Z89Orc95zznu+7/sKKSWf45OhWOwHSAR8LqR54HMhzQOfC2ke+FxI88DnQpoHrhohCSF2CyE6hBAXhBB/udjPMx3iatgnCSGUQCdwA2AFPgDuklK2LuqDRXG1jKSNwAUp5UUppR94AbhlkZ8pBtViP0AU+UDftPdW4NqPVhJC3AvcG329rqCgAJfLhdFoZGpGqNVqfD4f4XAYpVKJ0WjE4/GgVCpxOByEw2FcLpeI5+GuFiHNC1LKp4CnALRarfzbv/1bWltbCYVCpKeno1AocLlcrFixgubmZmw2G7m5udTW1nLmzBlSUlL41a9+FXe/V4uQ+oHCae8LomUfi9zcXEZGRlAoFKSkpNDd3Y3RaCQzM5POzk40Gg2HDx/mS1/6Evv372dgYACXy0UwGIz74a4WnfQBUCGEKBFCaIA7gVc/qcHk5CQjIyMkJSXhcrnIysqipKQEr9eLQqFgbGyM7OxsNm/ejNlsRqfTUVRUhNPpjPvhroqRJKUMCiG+BxwClMCvpZQtn9RGr9cTCAQoKSlBq9UyOTlJT08Pzz77LHl5eYyNjXHbbbdx6dIlLl++zNatWwkGgxw9ejTu57sqhAQgpXwdeH2+9bVaLVVVVXg8HtxuNx6Ph7fffpvMzEzsdjslJSXcc8896HQ6hoaGePbZZ+no6ECIuHQ2cBUJKV5MTk5itVrxer1cvHiR6upqvvKVr/D+++9TWFiIyWSiv7+f9PR0+vv76ejowGQysZB94dWik+JGKBRiaGgIvV5Pfn4+UkrMZjPZ2dkUFBRQWFhIWVkZ+/bt4+GHH6a4uJiamhoMBkPcfSXsSNJoNNTU1OB0OmPKur29nYyMDMbGxtDr9fT09LBnzx5uvPHG2Irn8/ni7ithR5Lf76empoYPP/yQ5cuXk5WVhU6nIxwOU1xcjE6nAyAcDhMIBAgEAnR0dBAIBOLuK2GFpNVqOXfuHKFQCLfbjcPhoK2tjcrKSsLhMG1tbYyMjOByuRgfH6eiogKz2UxqamrcfSXsdFOr1aSkpFBVVYXP56O7u5vS0lL6+vq45pprMBqNGAwGtFotSqUShUJBUVERLpcr7r4SVkhutxuj0ciqVavweDzk5+ezbNkyAoEAbW1tJCcnk5qait/vR6fT8a1vfYsvf/nLC5puCSskhUJBR0cHGo1m6tCKy+WipaUFlUrFpk2b6OzspKuri4mJCb797W8zNjaGUqmMu6+EFZLP5+P06dPU1tZisVhwOp0xC0BycjLvvfceFouFwsLIkVCtVuP1egmFQnH3lbBCmtplv/vuu/h8PsrKynA6nbS0tKBQKFizZg0TExO0tbVht9vp7u5GoVDgdrvj7ithhRQOh+nu7iYcDsem2+TkJHl5eQwPDwMwMTGBRqMhEAhgs9nIzMwkHA7H3VfCbgGklPh8vtgxw+fzodVqsdlssRGTlpaGTqcjIyMDIQR2u31BfSXsSFKpVGRnZzM+Pk4gEGBychKj0Ug4HCYpKYmBgQGuv/56zGYzjY2NeDweJicnY6Msrr6uwPN/JkhPT+fuu+9mYmICgNbWVkZHR+np6UFKSVpaGm1tbYRCIcxmMzU1NTgcDi5duhR3XwkrJJVKhdvtxul0olKpWLNmDcePH6e0tJScnBzy8/NZuXIlAC6Xi97eXiCyysWLhNVJ4XAYr9fLsmXLyMrKwuFwsHbtWkwmEyaTiX379pGXl4fFYqG/vx+Hw4FWq8Xj8cTdV8KOJCklDocDv9/PsmXLSEpKoqysjAMHDlBfX49KpaKzs5Pi4mLcbjdarRaFQhE7+MaDhB1JGo2GwsJCzGYzPp8Pk8nE0NAQe/bswel0Yjabsdls9PT04HK5eO+991i7du3Ssid5vV5cLhdKpZJAIIDH40GhUKBSqWL7I7/fT19fHyqVCrPZzJEjR5bWAddgMODz+fB4PKhUKsLhMGq1GiklSUlJsYPslNm2tbWV0tLSpWXjHhsbw+12Mz4+DoDT6SQ3Nzc2ssLhMBkZGUBko1leXk5ra2tsyxAPElYnKRQKiouLyc/PB4gdP7xeL/CfS30wGKSpqYlz585RVFS0oIuAhB1JwWAQh8OB2WwmEAigVqu59tprOXnyJCUlJQwPDyOlRK/Xk5KSwo9+9CPKy8u544474u4rYUdSVlYWhYWFTExMcP78eUZHR2lubmbt2rW0trbicDgA2LBhA1u3buXFF1/E7XZfmYsAIcSvhRDDQojz08rShBBvCSEs0d/maLkQQvwsSsRqFkLUTmvzjWh9ixDiG9PK1wkhzkXb/EzMU7MGg8GYDclms1FVVYXD4aC7u5vh4eGYPtLpdFRXV7NixQpOnTpFenp6HOKJYD4j6V+B3R8p+0vgsJSyAjgcfQ/wRaAi+nMv8EuICBV4mAidZiPw8JRgo3W+Pa3dR/uaE8FgkGAwiMlkYu/evbz22mtIKfn1r3/N2rVr8fv9AKSlpcXu5iYnJxe04/5UIUkpjwKjHym+BXg6+vpp4NZp5b+REZwCTEKIXGAX8JaUclRKOQa8BeyOfpYipTwlIxr1N9O+6xPh9Xoxm80kJSUhhKC8vJzS0lL+/M//nLKyMkwmExBZ2SYnJ7HZbKhUKlJSUubz9TOwUJ2ULaW0RV8PAtnR13ORsfI/pdw6R/mcEELcK4Q4I4Q4EwwGCYfD2Gw2/H4/ycnJnDt3DqfTSUlJSUwnZWdnU11djd/vJxgMMjk5Gfcf+19W3NER8JkQL6WUT0kp10sp109No6ysLNLT06msrIydy15//XXS0tIAcDgcDA8Ps379ejZs2PCZjqSh6FQh+nvKkvVxZKxPKi+Yo/xT4fP5YpeSk5OTKJVKsrKyuHjxIkVFRVgsFgAGBgbYt28fBw8epKen5zMdSa8CUyvUN4D/mFZ+T3SV2wSMR6flIeBGIYQ5qrBvBA5FP3MKITZFV7V7pn3XJ8Lv91NSUhIjSVRVVdHY2IjX6yUYDMaW+qKiIm688Uby8vJi1oB48ambSSHEvwHbgAwhhJXIKvUY8KIQ4ltALzC1Q3sd2ANcANzAHwNIKUeFEH9DhNEG8KiUcmox+DMiK6geeCP686kIhUKcOXMGg8HAxMQEVquVoqIiioqKEEJw44030tLSErNElpeXL1hIVwWPeyHIz8+XW7duZdeuXYTDYY4cOUJ/fz9VVVWo1WpSU1NRKpWMj4+TlJSEWq3G4/Hwr//6rwwMDMR1yk3YHTdAdXU1gUAAu93O1q1b+eY3v8nJkyc5fvw4O3fuBGDFihX09PSg0+nYsGEDKlX8J7GEPbup1WrsdjtOp5OcnByam5tZtmwZDz74IIcPH46tYn19kZ1HS0sLo6OjsU1mPEhYIQUCAcxmMxqNhrNnz7Jy5UoqKyvZv38/P/7xj9FoNACMj49TU1ODVqtFrVajUMQ/eRJWSEajkfT0dPr6+sjMzKSuro777rsPpVLJY489RkZGBkajkcbGRqqrqzGZTGRnZ6PX6+PuK2GFpNPp2LNnD1JKfvOb39DZ2UltbS0pKSmMjY1htVqprKyksbGR9PR0/uZv/oZQKMS//Mu/xN1XwgopEAgwMTFBMBjk1ltvZXBwkD179uBwOKipqaG3t5fm5mbKy8v50z/9U8bHx9HpdEuLfSulZGhoiMHBQcbHxyksLGT16tUcP36c9957j46ODgDeeOMNXnjhBZKTk5mYmFhahAmIHE28Xi92ux2Px4Pf72fLli2cOnUqJowLFy7wB3/wB/T19REOh5fWSAJiinjZsmUMDg7y5JNP0t7eTmZmJgUFkSPhxYsXKS4ujl0QJLIDTtxwOp2cOXOGysrKGFHC6/Wyc+dOdu/ezdDQEABbtmzB7XajVqtRq9UL2iclrJBCoRDl5eW8/PLLnDhxIuZX8tBDD9HZ2ck111wDRKjMaWlpTExM0NraGrtNiQcJu7pNLf3Jycls27aN/v5+/H4/aWlp9Pb2YrfbUavVDAwMUFhYyIsvvkhPTw/Jyclx95WwQpq6LwiHwzz11FMxD6Qbb7yRwsJChoeHUavV/OAHP2DNmjWMj49TVVXF8ePH4+4rYYXk8/lidqONGzeSk5OD1Wpl9erVM0gRN9xwA8FgkF27dhEKhfj3f//3uPtKWCGZzWYeeOABQqEQBoOB5557jr6+Pn71q1+xc+fO2Nlt2bJleL1efvOb33DHHXcwMjISd18JK6RQKMSjjz7Kli1byMzMpKGhgfHxcaqrq/F4PNTU1GCz2bBYLFgsFrxeL6dPn74ylsmrFYFAAI1GQ09PDz09PVRXV6PT6bBYLNTW1sZWMYPBwHXXXcfo6CiTk5NLi8ftdrsZGRlhcHAQiNiXwuEweXl55OXl0dDQQGlpKRC5o9u+fTvPPvvs0uJMKpVK9Ho9Op0uxk/KzMxEqVQSDAZj+ySIkFAHBga47rrrlpY9SaFQsH79esLhMIODg6SlpZGfn09bWxter5fly5fT2NgIwBe/+EWsVivDw8MkJSXF3VfCCkmtVqPRaGLM/+TkZA4dOoRWq6Wuro7Tp0/H7Nnnz5+nsbERk8m0IJ2UsNMtFArh9XoxGAwEg0FSUlJYt24dAwMDjI2NxS4nAUZHRzGZTAghFsR0S9iRlJyczMaNG7FarSiVSsLhMHV1dQwPD3P27Fnq6upi0STS0tLIyMiIxS6JFwkrJIVCwfnz50lJSUGn08WI7Lfccgt6vR69Xs+hQ4cA6O/vR0rJ+vXrr4w9SQhRKIQ4IoRoFUK0CCF+EC1fVCLX1IrV2tpKRkYGTqcTq9VKSUkJKpVqxp2/3+9ndHSUt95664q5vAeBH0kpa4BNwHeFEDUsMpHL7/eTnZ1NS0sLb7zxBg6HA6fTyQsvvEBfX9+MmCRFRUVUVFSwe/fuK2O+lVLapJSN0dcuoI0Ih2hRiVxDQ0OcPn2a6upqLBYLL7/8MhaLhaqqKmw2W8zoBtDZ2YndbufgwYMLGklx6SQhRDGwFnifRSByTY/EpdPpMBgMqFQqvvrVr+J2u1m+fDkA69at49prr+Xw4cNAZHWzWq2kpqYuyOg27y2AECIJeBm4T0o5IwjRZ0Xkmk7i0ul05OfnU1FRQUNDAzqdjpSUFJ577jkMBsMMAun69eu5+eabCYVCGI3GuPud10gSQqiJCOg5KeW+aPGQECJXSmmLg8i17SPl77BAIpdGo0GlUiGl5IYbbkCpVHL58mU2btzIk08+idFojDFwjx07Rl1dHWVlZVfm7BZdaf4ZaJNS/nTaR4tK5FKr1QQCAaqrqwmHw3g8HlwuF+3t7QwODtLW1harm5uby3/8x3+wf//+KxaGYwtwN3BOCNEULfsfLDKRa2JiArvdTmdnJ2q1mo6ODi5dusTKlStJTU2lubk5VvdLX/oS27dv52c/+1mMZRIPEpbEVVhYKHfu3InRaGRychIhBKWlpWRlZTE4OEgoFIqd+KcclNPS0nj++ecZHBxcGiQun89HcXExJpOJ3NxccnJyUKvVdHV1cc0118zYAgwODlJcXExhYeGCdFLCHkuMRiMrVqygtbWVS5cu0dPTw8TEBFqtFiEEZWVlsRP/hg0bOHnyJHffffeC/N0SdiSNjIzwk5/8hPb2diorK9m5cyePPfYYBoOBoqIidu/+z0272Wxm586dvPHGG0sr6k1OTg5/8Rd/QTgcZmhoKObBPTo6it1u57nnnosRtoqKirBarUxMTCzocjJhR5JCocBut3P48GH279/P5cuX+epXv8rDDz+MRqOZcfc2Pj6OWq3mu9/9biw+QFx9/T4f/LNEIBCgubmZc+fOUVlZyU033RRz69q1axdVVVWxuiMjI7S2tvLhhx/GPC3jQcIKaXx8nPfff5+77rqLYDDI4OAg586dY/Xq1Zw9e3bGpvGf//mfSUlJWZBVEhJYJ/n9furq6rj99tspLy/HYrFQWlrKwYMHUavVHDhwgFWrVsXqj4yM4PV6r+wB92pDUlJSLKjU2NgYPp8PvV6PyWRifHx8hjfSlA+cTqdbWiEUnU4nt956K8eOHePgwYMoFAqsVis+n4+qqipGR2f6MWZnZ7Njx46ltZk0mUw88cQTFBcXc9NNN8U8A44fP87ly5djtiWIXHUfPHiQ4eHhmEdlPEjYkQRw3333YTQaOXbsGJWVlRw+fJiWlhZaWlpmUGza29upqqrCYDAsKB53wgopHA4zPDzM5OQkubm5dHV1odfryc3NjR1mp+D1ehkZGWHLli0L6ithp1s4HOb06dO43W7sdjv5+fk4HA7UajUbNmyYUbe3tzdGRF1SxNJAIIDVasXj8TA+Pk5LSwvr168nKyuL5ubmmKMyRAx058+fx2KxLK2oN4FAAJVKRXV1dSwU0KVLl/j617+OVqudcc3d2dkJgMfjWZADTsKOJCklwWCQ9vZ2UlJS2LRpE2VlZTQ1NfHOO+/MUNDj4+OUl5ezevXqpUV2VyqVeL1e8vLySE5OZmhoCK1Wy7/927/R0tJCU1NTrO7tt9/OihUrMBqNS8uVKzc3l2uuuYaJiQk6OjoYGRkhMzOTtLQ0jEbjDAXd3NxMTk4OGRkZSyv2rcfjYcWKFRw/fpz8/HyysrLo6OhgaGiIpKSkGdfZg4OD2Gw21Gr1gu7dEna6BYNBzp07x8jICIFAAKfTid1ux+fzUVBQMEP3pKeno9frY+Fd40XCCikcDmO1WsnKyuLChQuUlZWRlpYW8/CeTkUOBAKx5FNLynwLUFBQEIsdefbs2ZgwgBlntNWrV3Px4kX8fv/SCsYphCAjIyM2ooqLi1GpVDidTjIyMmYY2BQKBSUlJbHwZfFiPtfcOiHEaSHE2SiJ65FoeYkQ4v0o8er/RRNFIYTQRt9fiH5ePO27/nu0vEMIsWtaedypFKd0i8FgoLa2lpqaGsrLyykvL0epVM5i2Wo0mgUpbZifTvIBO6SUq4E1RDhFm4D/BfwfKWU5MAZ8K1r/W8BYtPz/ROsRJX7dCawgQtJ6QgihFJFUir8gQv6qAe6K1v1EKJVKcnJyUKlUuFwu/H4/UspYqOmPRpKoqqqiurp6QTzu+ZC4pJRyauyqoz8S2AG8FC3/KIlritz1ElAfJULcArwgpfRJKbuJcAU28l9IpZiWloYQAovFgkqlisWeNBqNs+xGVquVEydOXDkf3Oh/vIkIveYtoAtwSCmn1tnpxKsYWSv6+TiQTvzkrrmeIxaJy+l08s4777Bhwwa+9rWvYTQa2bFjB9dffz233nor69evn/mHRvMGLATzEpKUMiSlXEOEO7QRqPqUJlcE00lcycnJFBUVEQ6HmZiY4Mknn6SwsJAtW7Zw9uxZli1bNqPtyMgI77777pXfAkgpHUKII8B1RLiQquhomU68miJxWYUQKiAVuMwnp0yMK5UiRBS3lJK8vDzMZjM//vGPGR0dRaVS8YUvfGHWtLpw4QJSygVFvZnP6pYphDBFX+uJ5KptA44Ae6PVPkrimiJ37QV+F6ULvgrcGV39SoiwbE+zgFSKEOFMXnfddXR1ddHa2kpWVhYqlYp3332XcDhMdnb2jPparZaampqYs2A8mI9Yc4Gno6uQAnhRSnlACNEKvCCE+FvgQyJsOKK/nxFCXCASevFOACllixDiRaCVCO35u1LKEICIM5UiRI4lJ06cYPny5TGqsk6nY82aNfj9fj744IMZ9evq6rh06dKViX0rpWwmwrj9aPlFIvrpo+Ve4Ksf811/B/zdHOVxpVKECKO2t7eX3Nxc3G43DQ0NeL1eTp48yerVq2cp7vT0dJKSkhZkT0rYHbdWqyUcDvPGG2/w6KOPMjo6SnFxMevXr8fn882K4N7Q0IAQYml5Tnq9Xvx+PzfffDPPP/88gUCAUChEUlISHo9n1r3/xo0bY9GW40XCWgGmEif09/fT29sbW7ncbjelpaWzFHdvby8//OEPl9YB12g00t/fz5133snKlSvRaDQ0NjZy3XXXEQwGsVqtM+ofPHiQ5cuXc/ny5bj7StiR5Ha7OXnyZCx14okTJyguLqatrY233nprxpUSRAK7XHvttUtrupnNZv7hH/6BY8eO4XA42LRpE++88w4ZGRls27aN7du3z2pTXl6+tAgTw8PDGAwGNm7ciF6v5/z583zhC18gEAjMyYtcu3Yter1+QZkCE3YkTV0jmc1mzp49S3Fxccyf5OzZs/zjP/7jjPrd3d0xz6Z4kbBC0uv1ZGZm0tvbS0FBAWfOnOGnP/0pR48eJTU1lR/96Ecz6j/44IMxm1O8SNjpFgqF0Ov1scC/paWl1NfXMzQ0hN/vn0Wxufbaa/nud7+7oFDTCetbYjKZ5F133cUdd9yB0+mksLCQAwcOcPjwYbq7u0lOTp6RfqO2tpaTJ0/yy1/+krGxsaXhW6LX6ykrK2N4eBghBKOjozidTnbv3s3y5cupra2dUV+r1VJYWLi0InFNTEyQkpJCWVkZKpWKpqYmdDodp06dorq6epYw+vv7eeuttxZEvUnYkSSl5Je//CVvvvkm3/nOd7h48SIDAwNs2LCB/v7+WRbI5uZmbrvttqW1mZy6lHzyySfR6/VcunQJo9FIcnIyFRUVs85uQghqamqWJvUmMzMTj8fD8PAw9fX1NDU1cfjw4RhxawqTk5M0NjYuLRIXRPgAjzzyCFVVVdx8880cPnyYixcvxsJKT0cgEKCwsHBpCSkUCpGSksLzzz9PQUEBXV1d1NTUsHnzZsrKymaZb3U6Henp6UsrGKdWq0WpVPLBBx/Q1taGUqmksLCQrq4uqqqqYmHMptDT00NmZubS2nErFAry8vLweDzs2bMHrVaLSqXi+9//Pg6Hg/T09BnmEr/fv/SOJX6/H7VaTVVVFdu2bUOpVHL//fdTWlrK3r17SU5OniGkrVu3YrFYllY87qkwHNXV1ZSVlfHEE0/gdDppbGzEZrPNWupdLhdHjx5dkJASdiQ5nU4yMzO57bbbuHTpEh6Ph+rqanJzc2lqasJiscRylwBs3ryZiYmJK7uZjJImPhRCHIi+X1R+UlpaGk8//TQnTpzggQceoLa2lrq6OrZs2cLExARf/vKXZ9S/cOEC4XD4ivu7/YDI9fYUFpWfpFAoePzxx3nmmWeor6/H7/fT29vLhQsXCAaDsxgkLpcLp9N5ZfhJAEKIAuAm4FfR94JF5idNTk5y7Nix2FHjww8/5OLFi6hUKhwOB7/97W9n1A8Gg7G7ungxX7E+DvwFMKX10lkEftJ0TE5O8sorr5CTk8OOHTv4q7/6K37xi1+wY8cOnE7njDAcAPX19bhcritDdhdCfAkYllI2CCG2xd3D7xEfjcTl9/tjHpGdnZ1MTk7icrmYnJycpaC7u7spKSm5YseSLcDNQogeIlNhB/B/ifKTonXm4icxT37SJ/GWZmA6iUuhUOB2u+nr6yM/P5+HHnqIqqoqiouLCQaDlJSUzGjb0NDAkSNHroznpJTyv0spC6SUxUQU7++klF9nkflJBoOBa6+9lurqaj788ENsNhvf+973SE1NJSsra5aj8tatWzGZTJ957NsHWER+0lSCcqvVSlNTE8eOHWPVqlX4/f5YoODp0SQuXLhASkrKgjaTCXsRUFZWJh955BEOHDgQ8+Revnw5o6OjscyA01eyjo4OvF4vDQ0N9Pb2Lo2LgIGBAV555RXuuecebDYbRqMRlUpFcXExOTk5sw6yRqMxlvQ8XiSskLKysti4cSPPPPMMAGNjY4yOjuLz+ejr65tldLPZbNTV1cVy5caDhD27jYyMoNFo+MY3vkF/fz/BYJAjR47Q1NREenp6zN9kCkNDQ7z33ntXbsd9NcLr9bJt2zYuXLjADTfcQEdHB9u3b+ehhx5CCMH58+dn1C8vL6enp2dpBS3PyMjgzJkz7Nq1C4fDwZ49e1i/fj0ajYY/+7M/myUMj8eDwWBYWk6BCoWClpYWDh06hMlkorAwsh9ta2vjO9/5Tuz9FFauXInRaFxQ3pKEFZLJZGLdunUUFRXx+OOPY7FY0Gq1rFmzhm9/+9t885vfnFFfrVZTXl6+tHxwgZhP7R//8R9TUVGBwWAgHA6zffv2WasbRMIDLWTHnbBCmkqNqFKpOHjwIHa7naGhIRobGxkeHp4zZ9Lu3buvfDzuqwlCCAKBAKmpqSxfvhyj0YjL5aKkpITXXnttRiSuKfh8vqW1BVAoFBQUFCCEQKfTYbVaY47Ie/bsmfOM9thjjy09wsTExARjY2NkZ2eTmpqKx+NBCIFer5/FTwJ46qmnlhZhQkqJlJKSkhI0Gg3Lly8nPz8/ZnjLzMyc1aahoWHpTTej0cjIyEjMRUuhUKBQKMjJyZnFKplqsxAhJaypZOXKlfKGG25gYGCAwcFBtm3bFouddPr0ad5///1ZbLfnn38eu93O6OhoXNvuhF3dppLfJSUlkZKSQkpKCl6vl7//+7+nq6uL0dHRGcRSiNzVfdTnZD5I2OkmosmjLBYLGzdupL6+nh/+8Ie0t7fHAk99FLt3756z/NOQsEJKSUkhFAqRk5ODxWLhBz/4AS0tLWRlZZGWljbLxg1QWlq6tA64AH/yJ39CdnZ2LDl5VVUVbrebrKysOanIQ0NDCwoSnLBCklLy85//nLy8PFatWkVGRkZsD6RQKMjKyprV5rnnnsNsNs8q/zQkrOKWUlJdXY3P56O8vJzly5fT19eHUqmku7t7zvwkBoMBm802x7d9MhJ2JKlUKsxmM1arlddeew23201hYSEOhwO9Xj/nsUSj0SytsK52u53s7Gyqq6t55JFHuHTpEhUVFZSXl7N9+/Y5aX82m4PSuNkAAArrSURBVG1p+buFw2F+97vfodVqOXLkCOFwGLvdDsD+/fvnPJakp6fPsljOB/Ol3vSISCa/JiHEmWjZomYKzMrKIj8/n/Pnz9PV1cWxY8diye8ee+yxOY8fBQUFDAwMzOdPnoF4RtJ2KeUaKeVU6IZFzRTodDppbm7ma1/7GiqVittvvx2bzYbX66W9vZ2VK1fOanPnnXd+5mT3Rc0UqNVqqaqq4oknnqCyspKjR4+ydetW6uvrCQaDvPPOO7PafPDBB3NOw0/DfIUkgTeFEA1RjhAsUqbAqSBTDocDlUpFVlYWhw4doqCggJycHBQKBR0dHZSVlc1qf++9917RtNPXSyn7hRBZwFtCiPbpH0oppRDiM8kUCDwFkJ+fL202GxUVFVRUVJCZmcnk5CSZmZmo1WoaGhpm7bpfeukl+vs/NTTTLMw3Eld/9Pcw8FsiOmUoOlUQ888U+HHlcWcKNBgMbN68maysLNRqNWNjY7z00ks0NDRQVlZGRUXFrDY5OTlXZgsghDAKIZKnXhPJ8HeeRc4U6HK56Ozs5IUXXuCll17i1KlT9Pb20tfXxy9+8Qu+8pWvzGpz9OjRKxZFORv4bXRVVgHPSykPCiE+YBEzBXq9XiYmJqipqYllBbTZbKSnp1NRUTEj1HTsD8nOXpCXUsJaJlNTU2Vubi4ul4va2lra29uRUqJWq9m0aRNFRUWzzCIdHR0cOnSIy5cvLw0Sl1arJTMzk2uuuYa+vj7uvfdeSktLyc/PJxAIzHmDu1A6YMJaAZRKJX/4h39Id3c35eXljI6Osn37dtLS0khNTaWoqIg333xzRptwOHxFtwBXHfR6fezmNhQKkZWVhVKp5Ny5cxQWFs7JQ9q0aRNvv/123H0lrJB8Ph/79+/H6/WyevVqPvjgAwKBAOfOnaO4uHhOo1tVVRVjY2Nx95WwQgqHw4yNjdHW1sabb77JP/3TP9HV1YXRaKS0tHROb6SRkZEF7ZMSVkgqlYqCggLy8/PJzMxk3759BAIBrr/+ep588kn27t07q83UFVS8SNjVTafTccstt1BbW0tTUxOTk5MEg0FMJhOZmZlzXh1t3bp1abmXOhwOHnzwQU6dOsXu3btxOBwolUqKi4t599136e7untXmlVdeWVrE0rS0NPbu3RszfUytcq+88go333zznPduu3fvJj09Pe6+ElZI4XAYjUYTy3FbUVHBxMQE77//Prm5uXR0dMxqM9dRZT5IWMXtdru5dOkSJ06coK6ujqSkJOrr6wmFQjQ0NMxJIM3NzV1a/CSv10tBQQE33HADq1evjuVQKioqori4mNzc3FltQqHQ0opYKoRg3bp1WCwWjh49SlFREefPn48RTisrK2e1OX369NIaSQDvvfceSUlJ6PV6Tp06hVqtJj8/H6/XS3V19az6TU1NCwpanrBCCoVC6HQ6bDYbubm51NfX09bWxsaNG3n44Ydjd3DTYTabl1Y87oyMDHp7ezEYDBw+fBi3283evXt57LHHWLVqFTt27JjV5p577uHs2bNx95WwQgoEArS3t+NwOAgEAni9XlwuF3fffTcADzzwwKyjyWuvvbYg6k3CCikUCjE6OorL5WJwcJCSkhIsFgsWiwWNRkNGRsasNkajcWnxkwKBAAaDASllLHLEqlWr6OrqQgjB6tWrZ7U5derUgrYACSuk5ORktm3bxu7du/n617/OHXfcgd1uJykpCavVyv333z+rzf33378gElfCCsnj8eDz+cjIyKCrqwuHw8H9999PXV0dt912G3v27JnV5sCBA0uLWBoOh5mcnMThcLBy5UosFgstLS1s2rSJjo4Orr/++lltVqxYwfj4eNx9JayQNBoN9fX1QCQwwh/90R/R09NDY2MjZrOZFStWzGoTDAaX1o5boVBw7NgxFAoFhYWFHD9+nLGxMUpLS5mYmGDfvn2z2kzlOom7r/lUEkKYhBAvCSHahRBtQojrFpvE5fP5cLvdVFVVkZSUxODgIPn5+TQ2NrJu3bo5jx9Wq/WK8pP+L3BQSlkFrCYSkWtRSVxTBv3W1la2bt2KSqUiPT2d2tpaTCbTnAdco9F4ZZLgCSFSga1EA7ZIKf1SSgeLTOLy+Xw4nU6WLVvGX//1X5OSkoLD4aCnp4fOzs45LyEXclMC8xtJJYAd+BcRCXz3qyi7ZFFJXG63O0ZRXr9+PV6vN2bC1el0lJeXz2q/kChcMD8hqYBa4JdSyrXAJP85tYAIiYsIG+6KYnqQqak8236/H5PJRGpqKi0tLRiNRnJycj52qb9SWwArYJVSvh99/xIRoS0qiUupVHL58mVefvnlWH5btVpNX18fw8PDH3srMtfN7qdhPpG4BoE+IcSUJqwnEihqUUlcCoWC6upq6uvrMRgMDA4OkpeXR3V1NVqt9mMdkq8kYeLPgeei4cQuEiFmKVhEEpfT6aSysjKWm3tgYACXy0UgECA/P/9jlfRC7t0SlsSl1+vlT37yE1599VV27dpFWloalZWVXLhwgVdffZWdO3fOCqMI8POf/xy73b50SFw9PT1UV1dz5swZXnnlFRQKBatXr8ZkMi3IJPJxSFijm16v5+LFi2zduhWj0UhPTw9vv/027e3tbN68mfb29jlva5eUA04wGMRms9Hd3Y1SqcRgMMQiAJ45c2ZBjjYfh4QdSSaTibvuuisWhmN4eJhbbrkFm83G2rVrsVqtMVbudCypRApCCKqqqhgfH4/lKjl79ixOp5Pf/va3HxsIeEnZuEOhEH19fVitVi5evIjdbketVuPxeDCbzTz++ONztltSkbi8Xi82m42SkhIMBgMajQa73c7w8DDBYHBOBxzgigctv+qQlJSE1+tlcHAQKSWjo6OxDBT33XffnG2uSALzqxWBQCCmXxwOB1qtloyMDPR6PX6/n6effnrOdksqeMLUiFi2bBlnzpxBp9MhpeTw4cPs2rWLzZs3z9luSeUtmdoUdnV1sWHDBnp7e7FarYRCIV599VWWLVs2Z7uFjKSEFZLP58PlcqFUKjEajaSnp2MymfD5fIyNjZGSkjJnDIAlpbgNBgMrVqwgKyuLkydP8vrrr6PT6airqyMzM/NjN40LceVKWCEB/PSnP+Wll17i0UcfJRgMIqXk1KlTZGRkzEksBeb0Xvo0JKyQxsfH+f73v09/fz/hcJibbrqJYDBIRUUFZWVlH2umXYidO2HtSUIIFzD3cIkgA5gdkROKpJRx+b0nrOIGOqYFcpgFIcSZT/o8HiTsdPss8bmQ5oFEFtJT/8XP542EVdyfJRJ5JH1m+FxI80DCCEkIUSiEOCKE6BVC+IQQdiHEXwoh/loI0S8iAbCahBB7prX5/WQmnIpGfLX/ALnAeqALWAV0Au1Esgz+tznq1wBnAS0RZkwXkVxNyujrUkATrVPzSX0nzEiKcgbURLIKniNCJDsOzGZrRfB7y0yYMEKKIp8IeaMYWAucAlKA70Wph7+exp77vWUmTDQhQeQo9TJwH+AFzgFlwBrABvzv33eHiSakQeBLwHNSyn1EuEwXpJQhKWUY+Cci0wl+D5kJY1hshRyH4hbAM0QSfZbwn0q3blqd+4noIYikkp2uuC8SUdqq6Ovp37HiE/te7D8+DiFdT4Ry2E1kmvmIEFqfITLlmokQyHKntfmfRFayDuCL08r3EFkdu4D/+Wl9f34smQcSTSctCj4X0jzwuZDmgc+FNA98LqR54HMhzQOfC2ke+P+DEDWKOfF4UAAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]}]}